{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech to Text\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Select device correctly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Load transcriber\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-tiny.en\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "    print(f\"Transcribing {file_path}...\")\n",
    "\n",
    "    import librosa\n",
    "\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    result = transcriber(audio)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Run\n",
    "\n",
    "text=transcribe_audio(r\"audio test/OAF_back_angry.wav\")\n",
    "print(text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorization\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"Gemini_api_key\")\n",
    "print(api_key)\n",
    "llm = GoogleGenerativeAI(api_key=api_key, model=\"gemini-2.0-flash\")\n",
    "llm\n",
    "\n",
    "llm.invoke(\"Hi\")\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "# structured output parser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Category(BaseModel):\n",
    "    category: str = Field(description=\"The category of the input text\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Category)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    },\n",
    "    template=\"\"\"\n",
    "You are MemoAI, an intelligent personal memory assistant.\n",
    "\n",
    "Your task is to analyze the given input text (spoken or written thought)\n",
    "and classify it into the most appropriate category or categories\n",
    "based on the user's intent and context.\n",
    "\n",
    "CRITICAL RULES (follow strictly):\n",
    "- If the input is a short command, system instruction, test phrase, or greeting\n",
    "  (e.g., \"say the word back\", \"repeat this\", \"hello\", \"good morning\"),\n",
    "  ALWAYS classify it as **General**.\n",
    "- Do NOT classify commands or repetition as Learning & Growth.\n",
    "- Learning & Growth applies ONLY to intentional self-improvement or study.\n",
    "\n",
    "Input:\n",
    "{input}\n",
    "\n",
    "Categories:\n",
    "\n",
    "1. Daily Life\n",
    "2. Work & Meetings\n",
    "3. Learning & Growth\n",
    "4. Health & Fitness\n",
    "5. Money & Shopping\n",
    "6. Entertainment & Leisure\n",
    "7. Ideas & Creativity\n",
    "8. General\n",
    "\n",
    "Instructions:\n",
    "- Choose MULTIPLE categories only if clearly applicable\n",
    "- Otherwise choose ONE best category\n",
    "- Output only the category name(s)\n",
    "- No explanations\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain  = prompt|llm|parser\n",
    "text\n",
    "chain.invoke(text).category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4731fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image based storage and retrieval\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"Gemini_api_key\")\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def gemini_image_description(image_path):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        contents=[\n",
    "            \"Describe this image clearly. Mention shop type, items, and visual context.\",\n",
    "            image\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def build_multimodal_context(voice_text, image_description):\n",
    "    return f\"\"\"\n",
    "    Voice description: {voice_text}\n",
    "    Image description: {image_description}\n",
    "    \"\"\"\n",
    "\n",
    "def extract_metadata(context):\n",
    "    prompt = f\"\"\"\n",
    "    From the following context, extract structured data in JSON.\n",
    "\n",
    "    Required fields:\n",
    "    - object (what place or thing)\n",
    "    - place\n",
    "    - category\n",
    "    - tags (list)\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Return ONLY valid JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def build_search_text(voice_text, image_description, metadata_json):\n",
    "    return f\"\"\"\n",
    "    {voice_text}.\n",
    "    {image_description}.\n",
    "    {metadata_json}\n",
    "    \"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    return embedder.encode(text)\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = 384\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "memory = []\n",
    "\n",
    "def store_image_memory(embedding, record):\n",
    "    index.add(np.array([embedding]))\n",
    "    memory.append(record)\n",
    "\n",
    "def ingest_image(image_path, voice_text):\n",
    "    image_desc = gemini_image_description(image_path)\n",
    "\n",
    "    context = build_multimodal_context(voice_text, image_desc)\n",
    "    metadata = extract_metadata(context)\n",
    "\n",
    "    search_text = build_search_text(\n",
    "        voice_text, image_desc, metadata\n",
    "    )\n",
    "\n",
    "    embedding = get_embedding(search_text)\n",
    "\n",
    "    store_image_memory(embedding, {\n",
    "        \"image_path\": image_path,\n",
    "        \"voice_text\": voice_text,\n",
    "        \"image_description\": image_desc,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "\n",
    "    return \"Image memory stored\"\n",
    "\n",
    "def search_image(query_text):\n",
    "    query_embedding = get_embedding(query_text)\n",
    "\n",
    "    D, I = index.search(np.array([query_embedding]), k=1)\n",
    "    result = memory[I[0][0]]\n",
    "\n",
    "    return result[\"image_path\"]\n",
    "\n",
    "ingest_image(\n",
    "    image_path=\"image.png\",\n",
    "    voice_text=\"This is a sarees shop in Nandyal remember and give me when I ask for it later\"\n",
    ")\n",
    "\n",
    "img = search_image(\"Give me the image of sarees shop in Nandyal\")\n",
    "print(img)\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open(img)\n",
    "image.show()\n",
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyD_183prlg2ycO6Jd0bcxeljv6J-Tgcpvw\"\n",
    "\n",
    "url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent\"\n",
    "params = {\"key\": API_KEY}\n",
    "\n",
    "payload = {\n",
    "    \"contents\": [\n",
    "        {\n",
    "            \"parts\": [{\"text\": \"ping\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, params=params, json=payload)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
